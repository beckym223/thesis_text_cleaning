are intended to stretch to the limit the
meager supply of facts.
Since, as I said before, the publishers'
referees do a competent job, most modeltesting
kits described in professional
journals are internally consistent. However,
like the economic models they are
supposed to implement, the validity of
these statistical tools depends itself on the
acceptance of certain convenient assumptions
pertaining to stochastic properties of
the phenomena which the particular
models are intended to explain; assumptions
that can be seldom verified.
In no other field of empirical inquiry has
so massive and sophisticated a statistical
machinery been used with such indifferent
results. Nevertheless, theorists continue
to turn out model after model and mathematical
statisticians to devise complicated
procedures one after another. Most of these
are relegated to the stockpile without any
practical application or after only a perfunctory
demonstration exercise. Even
those used for a while soon fall out of favor,
not because the methods that supersede
them perform better, but because they
are new and different.
Continued preoccupation with imaginary,
hypothetical, rather than with
observable reality has gradually led to a
distortion of the informal valuation scale
used in our academic community to assess
and to rank the scientific performance of its
members. Empirical analysis, according to
this scale, gets a lower rating than formal
mathematical reasoning. Devising a new
statistical procedure, however tenuous,
that makes it possible to squeeze out one
more unknown parameter from a given
set of data, is judged a greater scientific
achievement than the successful search
for additional information that would
permit us to measure the magnitude of the
same parameter in a less ingenious, but
more reliable way. This despite the fact
that in all too many instances sophisticated
statistical analysis is performed on a
set of data whose exact meaning and
validity are unknown to the author or
rather so well known to him that at the
very end he warns the reader not to take
the material conclusions of the entire
''exercise" seriously.
A natural Darwinian feedback operating
through selection of academic personnel
contributes greatly to the perpetuation of
this state of affairs. The scoring system
that governs the distribution of rewards
must naturally affect the make-up of the
competing teams. Thus, it is not surprising
that the younger economists, particularly
those engaged in teaching and in academic
research, seem by now quite content with a
situation in which they can demonstrate
their prowess (and incidentally, advance
their careers) by building more and more
complicated mathematical models and
devising more and more sophisticated
methods of statistical inference without
ever engaging in empirical research. Complaints
about the lack of indispensable primary
data are heard from time to time,
but they don't sound very urgent. The
feeling of dissatisfaction with the present
state of our discipline which prompts me
to speak out so bluntly seems, alas, to be
shared by relatively few. Yet even those
few who do share it feel they can do little
to improve the situation. How could they?
In contrast to most physical sciences, we
study a system that is not only exceedingly
complex but is also in a state of constant
flux. I have in mind not the obvious change
in the variables, such as outputs, prices or
levels of employment, that our equations
are supposed to explain, but the basic
structural relationships described by the
form and the parameters of these equations.
In order to know what the shape of
these structural relationships actually are
at any given time, we have to keep them
under continuous surveillance.
By sinking the foundations of our ana-